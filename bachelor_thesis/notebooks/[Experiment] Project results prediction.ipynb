{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fassty/anaconda3/envs/deep_learning/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from string import punctuation\n",
    "from stop_words import get_stop_words\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import nltk\n",
    "\n",
    "from gensim.models import Word2Vec, Doc2Vec, FastText\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVR, SVR\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['w2v_d2v_kwords_lemmatize',\n",
       " 'tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_res_pos_2',\n",
       " 'ud_pipe_czech_text',\n",
       " 'tf_w2v_d2v_fast_eng_n_a_kwords_2',\n",
       " 'tf_w2v_d2v_fast_eng_n_a_kwords',\n",
       " 'tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_res_pos',\n",
       " 'ud_pipe_nazev_anotace_stopwords',\n",
       " 'd2v_w2v_kwords_nolemma']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(os.listdir('saved_models/'))\n",
    "\n",
    "exp_features = {\n",
    "    'w2v_d2v_kwords_lemmatize': ['Kód projektu', 'Klíčová slova'],\n",
    "    'd2v_w2v_kwords_nolemma': ['Kód projektu', 'Klíčová slova'],\n",
    "    'ud_pipe_czech_text': ['Kód projektu', 'Název česky', 'Anotace česky'],\n",
    "    'ud_pipe_nazev_anotace_stopwords': ['Kód projektu', 'Název česky', 'Anotace česky'],\n",
    "    'tf_w2v_d2v_fast_eng_n_a_kwords_2': ['Kód projektu', 'Název anglicky', 'Anotace anglicky', 'Klíčová slova'],\n",
    "    'tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_res_pos_2': ['Kód projektu' ,'Název česky', 'Anotace česky', 'Hlavní CEP obor', 'Podrobné informace o účastnících', 'Hlavní řešitelé', 'Poskytovatel']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_for_exp(exp_name):\n",
    "    orig_df = pd.read_csv('../data/TACR_Starfos_isvav_project.csv')\n",
    "    df = orig_df[exp_features[exp_name]]\n",
    "    df = df[~df.isna().any(axis=1)]\n",
    "    orig_df = orig_df.iloc[df.index]\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.reset_index().set_index('Kód projektu')\n",
    "    orig_df = orig_df.reset_index(drop=True)\n",
    "\n",
    "    results = pd.read_csv('../data/VaVaI_Projekty_s_vysledky.csv')\n",
    "    results = results.set_index('Kód projektu')\n",
    "    results = results[results.index.isin(df.index)]\n",
    "    df = df.loc[results.index]\n",
    "    df = df.set_index('index')\n",
    "    orig_df = orig_df.loc[df.index]\n",
    "    return orig_df, df.index, results\n",
    "\n",
    "def load_vectors_for_exp(exp_name):\n",
    "    root_path = os.path.join('saved_models', exp_name)\n",
    "    with open(os.path.join(root_path, 'vectors.pickle'), 'rb') as handle:\n",
    "        vectors = pickle.load(handle)\n",
    "    return vectors\n",
    "\n",
    "weights = {\n",
    "    'Jx': 2,\n",
    "    'Jimp': 9,\n",
    "    'Jost': 7,\n",
    "    'JSC': 7,\n",
    "    'D': 4,\n",
    "    'C': 3,\n",
    "    'O': 1,\n",
    "    'X': 1,\n",
    "    'B': 6,\n",
    "    'Vsouhrn': 2,\n",
    "    'Vx': 2,\n",
    "    'Vutaj': 2,\n",
    "    'Gfunk': 3,\n",
    "    'Gprot': 3,\n",
    "    'A': 4,\n",
    "    'Nmap': 4,\n",
    "    'NmetC': 2,\n",
    "    'NmetS': 2,\n",
    "    'NmetA': 2,\n",
    "    'Npam': 2,\n",
    "    'Nlec': 2,\n",
    "    'W': 2,\n",
    "    'M': 2,\n",
    "    'R': 4,\n",
    "    'Fuzit': 3,\n",
    "    'Fprum': 3,\n",
    "    'Ztech': 4,\n",
    "    'Zpolop': 4,\n",
    "    'Zodru': 4,\n",
    "    'Zx': 1,\n",
    "    'Zplem': 2,\n",
    "    'P': 8,\n",
    "    'Enekrit': 2,\n",
    "    'Ekrit': 3,\n",
    "    'Hleg': 2,\n",
    "    'Hneleg': 2,\n",
    "    'Hkonc': 2,\n",
    "    'Sdb': 4\n",
    "}\n",
    "\n",
    "def assign_weights(res):\n",
    "    if len(res) == 0: \n",
    "        return 0\n",
    "    sres = res.split(';')\n",
    "    return sum([weights[r.strip()] for r in sres])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>model_name</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>w2v_d2v_kwords_lemmatize</td>\n",
       "      <td>tf-idf_(1,1)</td>\n",
       "      <td>0.018325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>w2v_d2v_kwords_lemmatize</td>\n",
       "      <td>tf-idf_(1,2)</td>\n",
       "      <td>0.018325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>w2v_d2v_kwords_lemmatize</td>\n",
       "      <td>tf-idf_(1,3)</td>\n",
       "      <td>0.018325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>w2v_d2v_kwords_lemmatize</td>\n",
       "      <td>tf-idf_(1,4)</td>\n",
       "      <td>0.018325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>w2v_d2v_kwords_lemmatize</td>\n",
       "      <td>tf-idf_(1,5)</td>\n",
       "      <td>0.018325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>doc2vec_100_50</td>\n",
       "      <td>0.058644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>doc2vec_200_5</td>\n",
       "      <td>0.055208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>doc2vec_200_10</td>\n",
       "      <td>0.059472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>doc2vec_200_25</td>\n",
       "      <td>0.059508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>doc2vec_200_50</td>\n",
       "      <td>0.056738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>335 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       experiment_name      model_name  \\\n",
       "0                             w2v_d2v_kwords_lemmatize    tf-idf_(1,1)   \n",
       "1                             w2v_d2v_kwords_lemmatize    tf-idf_(1,2)   \n",
       "2                             w2v_d2v_kwords_lemmatize    tf-idf_(1,3)   \n",
       "3                             w2v_d2v_kwords_lemmatize    tf-idf_(1,4)   \n",
       "4                             w2v_d2v_kwords_lemmatize    tf-idf_(1,5)   \n",
       "..                                                 ...             ...   \n",
       "330  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...  doc2vec_100_50   \n",
       "331  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...   doc2vec_200_5   \n",
       "332  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...  doc2vec_200_10   \n",
       "333  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...  doc2vec_200_25   \n",
       "334  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...  doc2vec_200_50   \n",
       "\n",
       "        score  \n",
       "0    0.018325  \n",
       "1    0.018325  \n",
       "2    0.018325  \n",
       "3    0.018325  \n",
       "4    0.018325  \n",
       "..        ...  \n",
       "330  0.058644  \n",
       "331  0.055208  \n",
       "332  0.059472  \n",
       "333  0.059508  \n",
       "334  0.056738  \n",
       "\n",
       "[335 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = defaultdict(list)\n",
    "for exp_name in exp_features.keys():\n",
    "    orig_data, idxs, results = load_data_for_exp(exp_name)\n",
    "    vectors = load_vectors_for_exp(exp_name)\n",
    "    results['VycetVysledku'] = results['VycetVysledku'].fillna('') \n",
    "    results['VahaEx'] = results['VycetVysledku'].apply(assign_weights)\n",
    "    results['Vaha'] = results['VycetVysledku'].apply(lambda x: len(x.split(';')))\n",
    "    \n",
    "    for model_name, vector in vectors.items():\n",
    "        X = vector[idxs]\n",
    "        y = results['VahaEx'].to_numpy()\n",
    "\n",
    "        if 'tf-idf' in model_name:\n",
    "            scaler = StandardScaler(with_mean=False)\n",
    "        else:\n",
    "            scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        model = LinearRegression()\n",
    "        model = model.fit(X_train, y_train)\n",
    "\n",
    "        res['experiment_name'].append(exp_name)\n",
    "        res['model_name'].append(model_name)\n",
    "        res['score'].append(model.score(X_test, y_test))\n",
    "        \n",
    "results_df = pd.DataFrame(res)\n",
    "results_df.to_csv('linear_regression_res_weighted.csv')\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>model_name</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>fasttext_200_10</td>\n",
       "      <td>0.105001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>fasttext_200_5</td>\n",
       "      <td>0.101960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>fasttext_200_25</td>\n",
       "      <td>0.099298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>fasttext_200_50</td>\n",
       "      <td>0.094370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>word2vec_200_10</td>\n",
       "      <td>0.089465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>word2vec_200_25</td>\n",
       "      <td>0.089239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>fasttext_100_50</td>\n",
       "      <td>0.086098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>fasttext_100_5</td>\n",
       "      <td>0.084292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>word2vec_200_5</td>\n",
       "      <td>0.084077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>word2vec_200_50</td>\n",
       "      <td>0.083465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>fasttext_100_10</td>\n",
       "      <td>0.082979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>fasttext_100_25</td>\n",
       "      <td>0.080619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>word2vec_100_25</td>\n",
       "      <td>0.079984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>word2vec_100_50</td>\n",
       "      <td>0.077580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>word2vec_100_10</td>\n",
       "      <td>0.075424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>word2vec_100_5</td>\n",
       "      <td>0.071803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>ud_pipe_czech_text</td>\n",
       "      <td>word2vec_200_10</td>\n",
       "      <td>0.070090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>fasttext_50_10</td>\n",
       "      <td>0.068514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>ud_pipe_czech_text</td>\n",
       "      <td>word2vec_200_25</td>\n",
       "      <td>0.068046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>word2vec_50_25</td>\n",
       "      <td>0.067858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>fasttext_50_50</td>\n",
       "      <td>0.066411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>fasttext_50_25</td>\n",
       "      <td>0.066247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>ud_pipe_czech_text</td>\n",
       "      <td>word2vec_200_50</td>\n",
       "      <td>0.064707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>doc2vec_100_10</td>\n",
       "      <td>0.064690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>ud_pipe_nazev_anotace_stopwords</td>\n",
       "      <td>word2vec_200_10</td>\n",
       "      <td>0.064151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       experiment_name       model_name  \\\n",
       "316  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...  fasttext_200_10   \n",
       "315  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...   fasttext_200_5   \n",
       "317  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...  fasttext_200_25   \n",
       "318  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...  fasttext_200_50   \n",
       "300  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...  word2vec_200_10   \n",
       "301  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...  word2vec_200_25   \n",
       "314  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...  fasttext_100_50   \n",
       "311  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...   fasttext_100_5   \n",
       "299  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...   word2vec_200_5   \n",
       "302  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...  word2vec_200_50   \n",
       "312  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...  fasttext_100_10   \n",
       "313  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...  fasttext_100_25   \n",
       "297  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...  word2vec_100_25   \n",
       "298  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...  word2vec_100_50   \n",
       "296  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...  word2vec_100_10   \n",
       "295  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...   word2vec_100_5   \n",
       "122                                 ud_pipe_czech_text  word2vec_200_10   \n",
       "308  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...   fasttext_50_10   \n",
       "123                                 ud_pipe_czech_text  word2vec_200_25   \n",
       "293  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...   word2vec_50_25   \n",
       "310  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...   fasttext_50_50   \n",
       "309  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...   fasttext_50_25   \n",
       "124                                 ud_pipe_czech_text  word2vec_200_50   \n",
       "328  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...   doc2vec_100_10   \n",
       "174                    ud_pipe_nazev_anotace_stopwords  word2vec_200_10   \n",
       "\n",
       "        score  \n",
       "316  0.105001  \n",
       "315  0.101960  \n",
       "317  0.099298  \n",
       "318  0.094370  \n",
       "300  0.089465  \n",
       "301  0.089239  \n",
       "314  0.086098  \n",
       "311  0.084292  \n",
       "299  0.084077  \n",
       "302  0.083465  \n",
       "312  0.082979  \n",
       "313  0.080619  \n",
       "297  0.079984  \n",
       "298  0.077580  \n",
       "296  0.075424  \n",
       "295  0.071803  \n",
       "122  0.070090  \n",
       "308  0.068514  \n",
       "123  0.068046  \n",
       "293  0.067858  \n",
       "310  0.066411  \n",
       "309  0.066247  \n",
       "124  0.064707  \n",
       "328  0.064690  \n",
       "174  0.064151  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.nlargest(25, 'score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10500050618608348"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['score'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = defaultdict(list)\n",
    "for exp_name in exp_features.keys():\n",
    "    orig_data, idxs, results = load_data_for_exp(exp_name)\n",
    "    vectors = load_vectors_for_exp(exp_name)\n",
    "    results['VycetVysledku'] = results['VycetVysledku'].fillna('') \n",
    "    results['VahaEx'] = results['VycetVysledku'].apply(assign_weights)\n",
    "    results['Vaha'] = results['VycetVysledku'].apply(lambda x: len(x.split(';')))\n",
    "    \n",
    "    for model_name, vector in vectors.items():\n",
    "        X = vector[idxs]\n",
    "        y = results['Vaha'].to_numpy()\n",
    "\n",
    "        if 'tf-idf' in model_name:\n",
    "            scaler = StandardScaler(with_mean=False)\n",
    "        else:\n",
    "            scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        model = MLPRegressor(hidden_layer_sizes=(128, 64), activation='tanh', random_state=42)\n",
    "        model = model.fit(X_train, y_train)\n",
    "\n",
    "        res['experiment_name'].append(exp_name)\n",
    "        res['model_name'].append(model_name)\n",
    "        res['score'].append(model.score(X_test, y_test))\n",
    "        \n",
    "        \n",
    "results_df = pd.DataFrame(res)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>model_name</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>w2v_d2v_kwords_lemmatize</td>\n",
       "      <td>tf-idf_(1,1)</td>\n",
       "      <td>0.033645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>w2v_d2v_kwords_lemmatize</td>\n",
       "      <td>tf-idf_(1,2)</td>\n",
       "      <td>0.033645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>w2v_d2v_kwords_lemmatize</td>\n",
       "      <td>tf-idf_(1,3)</td>\n",
       "      <td>0.033645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>w2v_d2v_kwords_lemmatize</td>\n",
       "      <td>tf-idf_(1,4)</td>\n",
       "      <td>0.033645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>w2v_d2v_kwords_lemmatize</td>\n",
       "      <td>tf-idf_(1,5)</td>\n",
       "      <td>0.033645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>w2v_d2v_kwords_lemmatize</td>\n",
       "      <td>tf-idf_(2,2)</td>\n",
       "      <td>0.033645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>w2v_d2v_kwords_lemmatize</td>\n",
       "      <td>tf-idf_(2,3)</td>\n",
       "      <td>0.033645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>w2v_d2v_kwords_lemmatize</td>\n",
       "      <td>tf-idf_(2,4)</td>\n",
       "      <td>0.033645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>w2v_d2v_kwords_lemmatize</td>\n",
       "      <td>tf-idf_(2,5)</td>\n",
       "      <td>0.033645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>w2v_d2v_kwords_lemmatize</td>\n",
       "      <td>tf-idf_(3,3)</td>\n",
       "      <td>0.033645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>w2v_d2v_kwords_lemmatize</td>\n",
       "      <td>tf-idf_(3,4)</td>\n",
       "      <td>0.033645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>w2v_d2v_kwords_lemmatize</td>\n",
       "      <td>tf-idf_(3,5)</td>\n",
       "      <td>0.033645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>w2v_d2v_kwords_lemmatize</td>\n",
       "      <td>tf-idf_(4,4)</td>\n",
       "      <td>0.033645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>w2v_d2v_kwords_lemmatize</td>\n",
       "      <td>tf-idf_(4,5)</td>\n",
       "      <td>0.033645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>w2v_d2v_kwords_lemmatize</td>\n",
       "      <td>tf-idf_(5,5)</td>\n",
       "      <td>0.033645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>tf-idf_(1,1)</td>\n",
       "      <td>0.028107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>tf-idf_(1,2)</td>\n",
       "      <td>0.028107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>tf-idf_(1,3)</td>\n",
       "      <td>0.028107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>tf-idf_(1,4)</td>\n",
       "      <td>0.028107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>tf-idf_(1,5)</td>\n",
       "      <td>0.028107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>tf-idf_(2,2)</td>\n",
       "      <td>0.028107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>tf-idf_(2,3)</td>\n",
       "      <td>0.028107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>tf-idf_(2,4)</td>\n",
       "      <td>0.028107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>tf-idf_(2,5)</td>\n",
       "      <td>0.028107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...</td>\n",
       "      <td>tf-idf_(3,3)</td>\n",
       "      <td>0.028107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       experiment_name    model_name     score\n",
       "0                             w2v_d2v_kwords_lemmatize  tf-idf_(1,1)  0.033645\n",
       "1                             w2v_d2v_kwords_lemmatize  tf-idf_(1,2)  0.033645\n",
       "2                             w2v_d2v_kwords_lemmatize  tf-idf_(1,3)  0.033645\n",
       "3                             w2v_d2v_kwords_lemmatize  tf-idf_(1,4)  0.033645\n",
       "4                             w2v_d2v_kwords_lemmatize  tf-idf_(1,5)  0.033645\n",
       "5                             w2v_d2v_kwords_lemmatize  tf-idf_(2,2)  0.033645\n",
       "6                             w2v_d2v_kwords_lemmatize  tf-idf_(2,3)  0.033645\n",
       "7                             w2v_d2v_kwords_lemmatize  tf-idf_(2,4)  0.033645\n",
       "8                             w2v_d2v_kwords_lemmatize  tf-idf_(2,5)  0.033645\n",
       "9                             w2v_d2v_kwords_lemmatize  tf-idf_(3,3)  0.033645\n",
       "10                            w2v_d2v_kwords_lemmatize  tf-idf_(3,4)  0.033645\n",
       "11                            w2v_d2v_kwords_lemmatize  tf-idf_(3,5)  0.033645\n",
       "12                            w2v_d2v_kwords_lemmatize  tf-idf_(4,4)  0.033645\n",
       "13                            w2v_d2v_kwords_lemmatize  tf-idf_(4,5)  0.033645\n",
       "14                            w2v_d2v_kwords_lemmatize  tf-idf_(5,5)  0.033645\n",
       "272  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...  tf-idf_(1,1)  0.028107\n",
       "273  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...  tf-idf_(1,2)  0.028107\n",
       "274  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...  tf-idf_(1,3)  0.028107\n",
       "275  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...  tf-idf_(1,4)  0.028107\n",
       "276  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...  tf-idf_(1,5)  0.028107\n",
       "277  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...  tf-idf_(2,2)  0.028107\n",
       "278  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...  tf-idf_(2,3)  0.028107\n",
       "279  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...  tf-idf_(2,4)  0.028107\n",
       "280  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...  tf-idf_(2,5)  0.028107\n",
       "281  tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_re...  tf-idf_(3,3)  0.028107"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.nlargest(25, 'score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 81 candidates, totalling 324 fits\n",
      "[CV 1/4] END max_depth=3, min_samples_leaf=50, min_samples_split=5, n_estimators=100; total time=  17.6s\n",
      "[CV 2/4] END max_depth=3, min_samples_leaf=50, min_samples_split=5, n_estimators=100; total time= 1.0min\n",
      "[CV 3/4] END max_depth=3, min_samples_leaf=50, min_samples_split=5, n_estimators=100; total time=  32.6s\n",
      "[CV 4/4] END max_depth=3, min_samples_leaf=50, min_samples_split=5, n_estimators=100; total time=  34.8s\n",
      "[CV 1/4] END max_depth=3, min_samples_leaf=50, min_samples_split=5, n_estimators=500; total time=  17.3s\n",
      "[CV 2/4] END max_depth=3, min_samples_leaf=50, min_samples_split=5, n_estimators=500; total time= 1.4min\n",
      "[CV 3/4] END max_depth=3, min_samples_leaf=50, min_samples_split=5, n_estimators=500; total time=  31.8s\n",
      "[CV 4/4] END max_depth=3, min_samples_leaf=50, min_samples_split=5, n_estimators=500; total time=  34.2s\n",
      "[CV 1/4] END max_depth=3, min_samples_leaf=50, min_samples_split=5, n_estimators=1000; total time=  17.1s\n",
      "[CV 2/4] END max_depth=3, min_samples_leaf=50, min_samples_split=5, n_estimators=1000; total time= 1.4min\n",
      "[CV 3/4] END max_depth=3, min_samples_leaf=50, min_samples_split=5, n_estimators=1000; total time=  31.6s\n",
      "[CV 4/4] END max_depth=3, min_samples_leaf=50, min_samples_split=5, n_estimators=1000; total time=  34.4s\n",
      "[CV 1/4] END max_depth=3, min_samples_leaf=50, min_samples_split=25, n_estimators=100; total time=  17.3s\n",
      "[CV 2/4] END max_depth=3, min_samples_leaf=50, min_samples_split=25, n_estimators=100; total time= 1.0min\n",
      "[CV 3/4] END max_depth=3, min_samples_leaf=50, min_samples_split=25, n_estimators=100; total time=  32.7s\n",
      "[CV 4/4] END max_depth=3, min_samples_leaf=50, min_samples_split=25, n_estimators=100; total time=  34.8s\n",
      "[CV 1/4] END max_depth=3, min_samples_leaf=50, min_samples_split=25, n_estimators=500; total time=  17.2s\n",
      "[CV 2/4] END max_depth=3, min_samples_leaf=50, min_samples_split=25, n_estimators=500; total time= 1.4min\n",
      "[CV 3/4] END max_depth=3, min_samples_leaf=50, min_samples_split=25, n_estimators=500; total time=  31.9s\n",
      "[CV 4/4] END max_depth=3, min_samples_leaf=50, min_samples_split=25, n_estimators=500; total time=  34.3s\n",
      "[CV 1/4] END max_depth=3, min_samples_leaf=50, min_samples_split=25, n_estimators=1000; total time=  17.1s\n",
      "[CV 2/4] END max_depth=3, min_samples_leaf=50, min_samples_split=25, n_estimators=1000; total time= 1.4min\n",
      "[CV 3/4] END max_depth=3, min_samples_leaf=50, min_samples_split=25, n_estimators=1000; total time=  32.3s\n",
      "[CV 4/4] END max_depth=3, min_samples_leaf=50, min_samples_split=25, n_estimators=1000; total time=  35.5s\n",
      "[CV 1/4] END max_depth=3, min_samples_leaf=50, min_samples_split=50, n_estimators=100; total time=  17.3s\n",
      "[CV 2/4] END max_depth=3, min_samples_leaf=50, min_samples_split=50, n_estimators=100; total time= 1.0min\n",
      "[CV 3/4] END max_depth=3, min_samples_leaf=50, min_samples_split=50, n_estimators=100; total time=  32.0s\n",
      "[CV 4/4] END max_depth=3, min_samples_leaf=50, min_samples_split=50, n_estimators=100; total time=  34.9s\n",
      "[CV 1/4] END max_depth=3, min_samples_leaf=50, min_samples_split=50, n_estimators=500; total time=  17.5s\n",
      "[CV 2/4] END max_depth=3, min_samples_leaf=50, min_samples_split=50, n_estimators=500; total time= 1.4min\n",
      "[CV 3/4] END max_depth=3, min_samples_leaf=50, min_samples_split=50, n_estimators=500; total time=  32.1s\n",
      "[CV 4/4] END max_depth=3, min_samples_leaf=50, min_samples_split=50, n_estimators=500; total time=  35.0s\n",
      "[CV 1/4] END max_depth=3, min_samples_leaf=50, min_samples_split=50, n_estimators=1000; total time=  17.8s\n",
      "[CV 2/4] END max_depth=3, min_samples_leaf=50, min_samples_split=50, n_estimators=1000; total time= 1.4min\n",
      "[CV 3/4] END max_depth=3, min_samples_leaf=50, min_samples_split=50, n_estimators=1000; total time=  32.0s\n",
      "[CV 4/4] END max_depth=3, min_samples_leaf=50, min_samples_split=50, n_estimators=1000; total time=  34.4s\n",
      "[CV 1/4] END max_depth=3, min_samples_leaf=100, min_samples_split=5, n_estimators=100; total time=  12.7s\n",
      "[CV 2/4] END max_depth=3, min_samples_leaf=100, min_samples_split=5, n_estimators=100; total time= 1.0min\n",
      "[CV 3/4] END max_depth=3, min_samples_leaf=100, min_samples_split=5, n_estimators=100; total time=  28.4s\n",
      "[CV 4/4] END max_depth=3, min_samples_leaf=100, min_samples_split=5, n_estimators=100; total time=  46.7s\n",
      "[CV 1/4] END max_depth=3, min_samples_leaf=100, min_samples_split=5, n_estimators=500; total time=  13.0s\n",
      "[CV 2/4] END max_depth=3, min_samples_leaf=100, min_samples_split=5, n_estimators=500; total time= 1.4min\n",
      "[CV 3/4] END max_depth=3, min_samples_leaf=100, min_samples_split=5, n_estimators=500; total time=  28.6s\n",
      "[CV 4/4] END max_depth=3, min_samples_leaf=100, min_samples_split=5, n_estimators=500; total time=  45.7s\n",
      "[CV 1/4] END max_depth=3, min_samples_leaf=100, min_samples_split=5, n_estimators=1000; total time=  12.8s\n",
      "[CV 2/4] END max_depth=3, min_samples_leaf=100, min_samples_split=5, n_estimators=1000; total time= 1.4min\n",
      "[CV 3/4] END max_depth=3, min_samples_leaf=100, min_samples_split=5, n_estimators=1000; total time=  28.7s\n",
      "[CV 4/4] END max_depth=3, min_samples_leaf=100, min_samples_split=5, n_estimators=1000; total time=  46.1s\n",
      "[CV 1/4] END max_depth=3, min_samples_leaf=100, min_samples_split=25, n_estimators=100; total time=  13.0s\n",
      "[CV 2/4] END max_depth=3, min_samples_leaf=100, min_samples_split=25, n_estimators=100; total time= 1.0min\n",
      "[CV 3/4] END max_depth=3, min_samples_leaf=100, min_samples_split=25, n_estimators=100; total time=  28.8s\n",
      "[CV 4/4] END max_depth=3, min_samples_leaf=100, min_samples_split=25, n_estimators=100; total time=  45.3s\n",
      "[CV 1/4] END max_depth=3, min_samples_leaf=100, min_samples_split=25, n_estimators=500; total time=  12.7s\n",
      "[CV 2/4] END max_depth=3, min_samples_leaf=100, min_samples_split=25, n_estimators=500; total time= 1.4min\n",
      "[CV 3/4] END max_depth=3, min_samples_leaf=100, min_samples_split=25, n_estimators=500; total time=  28.6s\n",
      "[CV 4/4] END max_depth=3, min_samples_leaf=100, min_samples_split=25, n_estimators=500; total time=  45.6s\n",
      "[CV 1/4] END max_depth=3, min_samples_leaf=100, min_samples_split=25, n_estimators=1000; total time=  12.9s\n",
      "[CV 2/4] END max_depth=3, min_samples_leaf=100, min_samples_split=25, n_estimators=1000; total time= 1.4min\n",
      "[CV 3/4] END max_depth=3, min_samples_leaf=100, min_samples_split=25, n_estimators=1000; total time=  28.7s\n",
      "[CV 4/4] END max_depth=3, min_samples_leaf=100, min_samples_split=25, n_estimators=1000; total time=  45.9s\n",
      "[CV 1/4] END max_depth=3, min_samples_leaf=100, min_samples_split=50, n_estimators=100; total time=  12.8s\n",
      "[CV 2/4] END max_depth=3, min_samples_leaf=100, min_samples_split=50, n_estimators=100; total time= 1.0min\n",
      "[CV 3/4] END max_depth=3, min_samples_leaf=100, min_samples_split=50, n_estimators=100; total time=  28.6s\n",
      "[CV 4/4] END max_depth=3, min_samples_leaf=100, min_samples_split=50, n_estimators=100; total time=  46.0s\n",
      "[CV 1/4] END max_depth=3, min_samples_leaf=100, min_samples_split=50, n_estimators=500; total time=  13.0s\n",
      "[CV 2/4] END max_depth=3, min_samples_leaf=100, min_samples_split=50, n_estimators=500; total time= 1.4min\n",
      "[CV 3/4] END max_depth=3, min_samples_leaf=100, min_samples_split=50, n_estimators=500; total time=  28.6s\n",
      "[CV 4/4] END max_depth=3, min_samples_leaf=100, min_samples_split=50, n_estimators=500; total time=  46.0s\n",
      "[CV 1/4] END max_depth=3, min_samples_leaf=100, min_samples_split=50, n_estimators=1000; total time=  12.7s\n",
      "[CV 2/4] END max_depth=3, min_samples_leaf=100, min_samples_split=50, n_estimators=1000; total time= 1.4min\n",
      "[CV 3/4] END max_depth=3, min_samples_leaf=100, min_samples_split=50, n_estimators=1000; total time=  29.0s\n",
      "[CV 4/4] END max_depth=3, min_samples_leaf=100, min_samples_split=50, n_estimators=1000; total time=  45.7s\n",
      "[CV 1/4] END max_depth=3, min_samples_leaf=250, min_samples_split=5, n_estimators=100; total time=  30.3s\n",
      "[CV 2/4] END max_depth=3, min_samples_leaf=250, min_samples_split=5, n_estimators=100; total time=  59.5s\n",
      "[CV 3/4] END max_depth=3, min_samples_leaf=250, min_samples_split=5, n_estimators=100; total time=  41.2s\n",
      "[CV 4/4] END max_depth=3, min_samples_leaf=250, min_samples_split=5, n_estimators=100; total time=  38.2s\n",
      "[CV 1/4] END max_depth=3, min_samples_leaf=250, min_samples_split=5, n_estimators=500; total time=  30.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/4] END max_depth=3, min_samples_leaf=250, min_samples_split=5, n_estimators=500; total time= 1.9min\n",
      "[CV 3/4] END max_depth=3, min_samples_leaf=250, min_samples_split=5, n_estimators=500; total time=  41.0s\n",
      "[CV 4/4] END max_depth=3, min_samples_leaf=250, min_samples_split=5, n_estimators=500; total time=  38.4s\n",
      "[CV 1/4] END max_depth=3, min_samples_leaf=250, min_samples_split=5, n_estimators=1000; total time=  30.7s\n",
      "[CV 2/4] END max_depth=3, min_samples_leaf=250, min_samples_split=5, n_estimators=1000; total time= 1.9min\n",
      "[CV 3/4] END max_depth=3, min_samples_leaf=250, min_samples_split=5, n_estimators=1000; total time=  40.9s\n",
      "[CV 4/4] END max_depth=3, min_samples_leaf=250, min_samples_split=5, n_estimators=1000; total time=  38.1s\n",
      "[CV 1/4] END max_depth=3, min_samples_leaf=250, min_samples_split=25, n_estimators=100; total time=  30.4s\n",
      "[CV 2/4] END max_depth=3, min_samples_leaf=250, min_samples_split=25, n_estimators=100; total time=  59.7s\n",
      "[CV 3/4] END max_depth=3, min_samples_leaf=250, min_samples_split=25, n_estimators=100; total time=  41.2s\n",
      "[CV 4/4] END max_depth=3, min_samples_leaf=250, min_samples_split=25, n_estimators=100; total time=  37.9s\n",
      "[CV 1/4] END max_depth=3, min_samples_leaf=250, min_samples_split=25, n_estimators=500; total time=  30.3s\n",
      "[CV 2/4] END max_depth=3, min_samples_leaf=250, min_samples_split=25, n_estimators=500; total time= 1.8min\n",
      "[CV 3/4] END max_depth=3, min_samples_leaf=250, min_samples_split=25, n_estimators=500; total time=  41.1s\n",
      "[CV 4/4] END max_depth=3, min_samples_leaf=250, min_samples_split=25, n_estimators=500; total time=  37.9s\n",
      "[CV 1/4] END max_depth=3, min_samples_leaf=250, min_samples_split=25, n_estimators=1000; total time=  30.2s\n",
      "[CV 2/4] END max_depth=3, min_samples_leaf=250, min_samples_split=25, n_estimators=1000; total time= 1.8min\n",
      "[CV 3/4] END max_depth=3, min_samples_leaf=250, min_samples_split=25, n_estimators=1000; total time=  41.0s\n",
      "[CV 4/4] END max_depth=3, min_samples_leaf=250, min_samples_split=25, n_estimators=1000; total time=  37.9s\n",
      "[CV 1/4] END max_depth=3, min_samples_leaf=250, min_samples_split=50, n_estimators=100; total time=  30.2s\n",
      "[CV 2/4] END max_depth=3, min_samples_leaf=250, min_samples_split=50, n_estimators=100; total time=  59.2s\n",
      "[CV 3/4] END max_depth=3, min_samples_leaf=250, min_samples_split=50, n_estimators=100; total time=  40.8s\n",
      "[CV 4/4] END max_depth=3, min_samples_leaf=250, min_samples_split=50, n_estimators=100; total time=  37.9s\n",
      "[CV 1/4] END max_depth=3, min_samples_leaf=250, min_samples_split=50, n_estimators=500; total time=  30.3s\n",
      "[CV 2/4] END max_depth=3, min_samples_leaf=250, min_samples_split=50, n_estimators=500; total time= 1.8min\n",
      "[CV 3/4] END max_depth=3, min_samples_leaf=250, min_samples_split=50, n_estimators=500; total time=  40.9s\n",
      "[CV 4/4] END max_depth=3, min_samples_leaf=250, min_samples_split=50, n_estimators=500; total time=  37.9s\n",
      "[CV 1/4] END max_depth=3, min_samples_leaf=250, min_samples_split=50, n_estimators=1000; total time=  30.2s\n",
      "[CV 2/4] END max_depth=3, min_samples_leaf=250, min_samples_split=50, n_estimators=1000; total time= 1.8min\n",
      "[CV 3/4] END max_depth=3, min_samples_leaf=250, min_samples_split=50, n_estimators=1000; total time=  40.9s\n",
      "[CV 4/4] END max_depth=3, min_samples_leaf=250, min_samples_split=50, n_estimators=1000; total time=  38.5s\n",
      "[CV 1/4] END max_depth=10, min_samples_leaf=50, min_samples_split=5, n_estimators=100; total time= 1.5min\n",
      "[CV 2/4] END max_depth=10, min_samples_leaf=50, min_samples_split=5, n_estimators=100; total time= 2.3min\n",
      "[CV 3/4] END max_depth=10, min_samples_leaf=50, min_samples_split=5, n_estimators=100; total time= 2.5min\n",
      "[CV 4/4] END max_depth=10, min_samples_leaf=50, min_samples_split=5, n_estimators=100; total time= 1.1min\n",
      "[CV 1/4] END max_depth=10, min_samples_leaf=50, min_samples_split=5, n_estimators=500; total time= 1.5min\n",
      "[CV 2/4] END max_depth=10, min_samples_leaf=50, min_samples_split=5, n_estimators=500; total time= 2.4min\n",
      "[CV 3/4] END max_depth=10, min_samples_leaf=50, min_samples_split=5, n_estimators=500; total time= 2.5min\n",
      "[CV 4/4] END max_depth=10, min_samples_leaf=50, min_samples_split=5, n_estimators=500; total time= 1.1min\n",
      "[CV 1/4] END max_depth=10, min_samples_leaf=50, min_samples_split=5, n_estimators=1000; total time= 1.5min\n",
      "[CV 2/4] END max_depth=10, min_samples_leaf=50, min_samples_split=5, n_estimators=1000; total time= 2.3min\n",
      "[CV 3/4] END max_depth=10, min_samples_leaf=50, min_samples_split=5, n_estimators=1000; total time= 2.5min\n",
      "[CV 4/4] END max_depth=10, min_samples_leaf=50, min_samples_split=5, n_estimators=1000; total time= 1.1min\n",
      "[CV 1/4] END max_depth=10, min_samples_leaf=50, min_samples_split=25, n_estimators=100; total time= 1.5min\n",
      "[CV 2/4] END max_depth=10, min_samples_leaf=50, min_samples_split=25, n_estimators=100; total time= 2.3min\n",
      "[CV 3/4] END max_depth=10, min_samples_leaf=50, min_samples_split=25, n_estimators=100; total time= 2.5min\n",
      "[CV 4/4] END max_depth=10, min_samples_leaf=50, min_samples_split=25, n_estimators=100; total time= 1.1min\n",
      "[CV 1/4] END max_depth=10, min_samples_leaf=50, min_samples_split=25, n_estimators=500; total time= 1.5min\n",
      "[CV 2/4] END max_depth=10, min_samples_leaf=50, min_samples_split=25, n_estimators=500; total time= 2.3min\n",
      "[CV 3/4] END max_depth=10, min_samples_leaf=50, min_samples_split=25, n_estimators=500; total time= 2.5min\n",
      "[CV 4/4] END max_depth=10, min_samples_leaf=50, min_samples_split=25, n_estimators=500; total time= 1.1min\n",
      "[CV 1/4] END max_depth=10, min_samples_leaf=50, min_samples_split=25, n_estimators=1000; total time= 1.5min\n",
      "[CV 2/4] END max_depth=10, min_samples_leaf=50, min_samples_split=25, n_estimators=1000; total time= 2.3min\n",
      "[CV 3/4] END max_depth=10, min_samples_leaf=50, min_samples_split=25, n_estimators=1000; total time= 2.5min\n",
      "[CV 4/4] END max_depth=10, min_samples_leaf=50, min_samples_split=25, n_estimators=1000; total time= 1.1min\n",
      "[CV 1/4] END max_depth=10, min_samples_leaf=50, min_samples_split=50, n_estimators=100; total time= 1.5min\n",
      "[CV 2/4] END max_depth=10, min_samples_leaf=50, min_samples_split=50, n_estimators=100; total time= 2.3min\n",
      "[CV 3/4] END max_depth=10, min_samples_leaf=50, min_samples_split=50, n_estimators=100; total time= 2.5min\n",
      "[CV 4/4] END max_depth=10, min_samples_leaf=50, min_samples_split=50, n_estimators=100; total time= 1.1min\n",
      "[CV 1/4] END max_depth=10, min_samples_leaf=50, min_samples_split=50, n_estimators=500; total time= 1.5min\n",
      "[CV 2/4] END max_depth=10, min_samples_leaf=50, min_samples_split=50, n_estimators=500; total time= 2.5min\n",
      "[CV 3/4] END max_depth=10, min_samples_leaf=50, min_samples_split=50, n_estimators=500; total time= 2.7min\n",
      "[CV 4/4] END max_depth=10, min_samples_leaf=50, min_samples_split=50, n_estimators=500; total time= 1.1min\n",
      "[CV 1/4] END max_depth=10, min_samples_leaf=50, min_samples_split=50, n_estimators=1000; total time= 1.6min\n",
      "[CV 2/4] END max_depth=10, min_samples_leaf=50, min_samples_split=50, n_estimators=1000; total time= 2.5min\n",
      "[CV 3/4] END max_depth=10, min_samples_leaf=50, min_samples_split=50, n_estimators=1000; total time= 2.7min\n",
      "[CV 4/4] END max_depth=10, min_samples_leaf=50, min_samples_split=50, n_estimators=1000; total time= 1.1min\n",
      "[CV 1/4] END max_depth=10, min_samples_leaf=100, min_samples_split=5, n_estimators=100; total time= 1.0min\n",
      "[CV 2/4] END max_depth=10, min_samples_leaf=100, min_samples_split=5, n_estimators=100; total time= 2.4min\n",
      "[CV 3/4] END max_depth=10, min_samples_leaf=100, min_samples_split=5, n_estimators=100; total time= 2.6min\n",
      "[CV 4/4] END max_depth=10, min_samples_leaf=100, min_samples_split=5, n_estimators=100; total time= 1.3min\n",
      "[CV 1/4] END max_depth=10, min_samples_leaf=100, min_samples_split=5, n_estimators=500; total time= 1.0min\n",
      "[CV 2/4] END max_depth=10, min_samples_leaf=100, min_samples_split=5, n_estimators=500; total time= 2.3min\n",
      "[CV 3/4] END max_depth=10, min_samples_leaf=100, min_samples_split=5, n_estimators=500; total time= 2.6min\n",
      "[CV 4/4] END max_depth=10, min_samples_leaf=100, min_samples_split=5, n_estimators=500; total time= 1.3min\n",
      "[CV 1/4] END max_depth=10, min_samples_leaf=100, min_samples_split=5, n_estimators=1000; total time= 1.0min\n",
      "[CV 2/4] END max_depth=10, min_samples_leaf=100, min_samples_split=5, n_estimators=1000; total time= 2.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/4] END max_depth=10, min_samples_leaf=100, min_samples_split=5, n_estimators=1000; total time= 2.6min\n",
      "[CV 4/4] END max_depth=10, min_samples_leaf=100, min_samples_split=5, n_estimators=1000; total time= 1.3min\n",
      "[CV 1/4] END max_depth=10, min_samples_leaf=100, min_samples_split=25, n_estimators=100; total time= 1.0min\n",
      "[CV 2/4] END max_depth=10, min_samples_leaf=100, min_samples_split=25, n_estimators=100; total time= 2.3min\n",
      "[CV 3/4] END max_depth=10, min_samples_leaf=100, min_samples_split=25, n_estimators=100; total time= 2.5min\n",
      "[CV 4/4] END max_depth=10, min_samples_leaf=100, min_samples_split=25, n_estimators=100; total time= 1.3min\n",
      "[CV 1/4] END max_depth=10, min_samples_leaf=100, min_samples_split=25, n_estimators=500; total time=  59.2s\n",
      "[CV 2/4] END max_depth=10, min_samples_leaf=100, min_samples_split=25, n_estimators=500; total time= 2.3min\n",
      "[CV 3/4] END max_depth=10, min_samples_leaf=100, min_samples_split=25, n_estimators=500; total time= 2.5min\n",
      "[CV 4/4] END max_depth=10, min_samples_leaf=100, min_samples_split=25, n_estimators=500; total time= 1.2min\n",
      "[CV 1/4] END max_depth=10, min_samples_leaf=100, min_samples_split=25, n_estimators=1000; total time=  58.7s\n",
      "[CV 2/4] END max_depth=10, min_samples_leaf=100, min_samples_split=25, n_estimators=1000; total time= 2.3min\n",
      "[CV 3/4] END max_depth=10, min_samples_leaf=100, min_samples_split=25, n_estimators=1000; total time= 2.5min\n",
      "[CV 4/4] END max_depth=10, min_samples_leaf=100, min_samples_split=25, n_estimators=1000; total time= 1.2min\n",
      "[CV 1/4] END max_depth=10, min_samples_leaf=100, min_samples_split=50, n_estimators=100; total time=  58.6s\n",
      "[CV 2/4] END max_depth=10, min_samples_leaf=100, min_samples_split=50, n_estimators=100; total time= 2.3min\n",
      "[CV 3/4] END max_depth=10, min_samples_leaf=100, min_samples_split=50, n_estimators=100; total time= 2.5min\n",
      "[CV 4/4] END max_depth=10, min_samples_leaf=100, min_samples_split=50, n_estimators=100; total time= 1.2min\n",
      "[CV 1/4] END max_depth=10, min_samples_leaf=100, min_samples_split=50, n_estimators=500; total time=  56.9s\n",
      "[CV 2/4] END max_depth=10, min_samples_leaf=100, min_samples_split=50, n_estimators=500; total time= 2.2min\n",
      "[CV 3/4] END max_depth=10, min_samples_leaf=100, min_samples_split=50, n_estimators=500; total time= 2.5min\n",
      "[CV 4/4] END max_depth=10, min_samples_leaf=100, min_samples_split=50, n_estimators=500; total time= 1.2min\n",
      "[CV 1/4] END max_depth=10, min_samples_leaf=100, min_samples_split=50, n_estimators=1000; total time=  56.7s\n",
      "[CV 2/4] END max_depth=10, min_samples_leaf=100, min_samples_split=50, n_estimators=1000; total time= 2.2min\n",
      "[CV 3/4] END max_depth=10, min_samples_leaf=100, min_samples_split=50, n_estimators=1000; total time= 2.4min\n",
      "[CV 4/4] END max_depth=10, min_samples_leaf=100, min_samples_split=50, n_estimators=1000; total time= 1.2min\n",
      "[CV 1/4] END max_depth=10, min_samples_leaf=250, min_samples_split=5, n_estimators=100; total time= 1.4min\n",
      "[CV 2/4] END max_depth=10, min_samples_leaf=250, min_samples_split=5, n_estimators=100; total time= 2.7min\n",
      "[CV 3/4] END max_depth=10, min_samples_leaf=250, min_samples_split=5, n_estimators=100; total time= 1.9min\n",
      "[CV 4/4] END max_depth=10, min_samples_leaf=250, min_samples_split=5, n_estimators=100; total time= 2.6min\n",
      "[CV 1/4] END max_depth=10, min_samples_leaf=250, min_samples_split=5, n_estimators=500; total time= 1.4min\n",
      "[CV 2/4] END max_depth=10, min_samples_leaf=250, min_samples_split=5, n_estimators=500; total time= 3.5min\n",
      "[CV 3/4] END max_depth=10, min_samples_leaf=250, min_samples_split=5, n_estimators=500; total time= 1.9min\n",
      "[CV 4/4] END max_depth=10, min_samples_leaf=250, min_samples_split=5, n_estimators=500; total time= 2.6min\n",
      "[CV 1/4] END max_depth=10, min_samples_leaf=250, min_samples_split=5, n_estimators=1000; total time= 1.4min\n",
      "[CV 2/4] END max_depth=10, min_samples_leaf=250, min_samples_split=5, n_estimators=1000; total time= 3.5min\n",
      "[CV 3/4] END max_depth=10, min_samples_leaf=250, min_samples_split=5, n_estimators=1000; total time= 1.9min\n",
      "[CV 4/4] END max_depth=10, min_samples_leaf=250, min_samples_split=5, n_estimators=1000; total time= 2.6min\n",
      "[CV 1/4] END max_depth=10, min_samples_leaf=250, min_samples_split=25, n_estimators=100; total time= 1.4min\n",
      "[CV 2/4] END max_depth=10, min_samples_leaf=250, min_samples_split=25, n_estimators=100; total time= 2.7min\n",
      "[CV 3/4] END max_depth=10, min_samples_leaf=250, min_samples_split=25, n_estimators=100; total time= 1.9min\n",
      "[CV 4/4] END max_depth=10, min_samples_leaf=250, min_samples_split=25, n_estimators=100; total time= 2.6min\n",
      "[CV 1/4] END max_depth=10, min_samples_leaf=250, min_samples_split=25, n_estimators=500; total time= 1.4min\n",
      "[CV 2/4] END max_depth=10, min_samples_leaf=250, min_samples_split=25, n_estimators=500; total time= 3.6min\n",
      "[CV 3/4] END max_depth=10, min_samples_leaf=250, min_samples_split=25, n_estimators=500; total time= 2.0min\n",
      "[CV 4/4] END max_depth=10, min_samples_leaf=250, min_samples_split=25, n_estimators=500; total time= 2.7min\n",
      "[CV 1/4] END max_depth=10, min_samples_leaf=250, min_samples_split=25, n_estimators=1000; total time= 1.4min\n",
      "[CV 2/4] END max_depth=10, min_samples_leaf=250, min_samples_split=25, n_estimators=1000; total time= 3.6min\n",
      "[CV 3/4] END max_depth=10, min_samples_leaf=250, min_samples_split=25, n_estimators=1000; total time= 1.9min\n",
      "[CV 4/4] END max_depth=10, min_samples_leaf=250, min_samples_split=25, n_estimators=1000; total time= 2.6min\n",
      "[CV 1/4] END max_depth=10, min_samples_leaf=250, min_samples_split=50, n_estimators=100; total time= 1.4min\n",
      "[CV 2/4] END max_depth=10, min_samples_leaf=250, min_samples_split=50, n_estimators=100; total time= 2.8min\n",
      "[CV 3/4] END max_depth=10, min_samples_leaf=250, min_samples_split=50, n_estimators=100; total time= 2.0min\n",
      "[CV 4/4] END max_depth=10, min_samples_leaf=250, min_samples_split=50, n_estimators=100; total time= 2.7min\n",
      "[CV 1/4] END max_depth=10, min_samples_leaf=250, min_samples_split=50, n_estimators=500; total time= 1.5min\n",
      "[CV 2/4] END max_depth=10, min_samples_leaf=250, min_samples_split=50, n_estimators=500; total time= 3.5min\n",
      "[CV 3/4] END max_depth=10, min_samples_leaf=250, min_samples_split=50, n_estimators=500; total time= 1.9min\n",
      "[CV 4/4] END max_depth=10, min_samples_leaf=250, min_samples_split=50, n_estimators=500; total time= 2.7min\n",
      "[CV 1/4] END max_depth=10, min_samples_leaf=250, min_samples_split=50, n_estimators=1000; total time= 1.5min\n",
      "[CV 2/4] END max_depth=10, min_samples_leaf=250, min_samples_split=50, n_estimators=1000; total time= 3.7min\n",
      "[CV 3/4] END max_depth=10, min_samples_leaf=250, min_samples_split=50, n_estimators=1000; total time= 2.0min\n",
      "[CV 4/4] END max_depth=10, min_samples_leaf=250, min_samples_split=50, n_estimators=1000; total time= 2.8min\n",
      "[CV 1/4] END max_depth=25, min_samples_leaf=50, min_samples_split=5, n_estimators=100; total time= 1.3min\n",
      "[CV 2/4] END max_depth=25, min_samples_leaf=50, min_samples_split=5, n_estimators=100; total time= 6.6min\n",
      "[CV 3/4] END max_depth=25, min_samples_leaf=50, min_samples_split=5, n_estimators=100; total time= 1.8min\n",
      "[CV 4/4] END max_depth=25, min_samples_leaf=50, min_samples_split=5, n_estimators=100; total time= 5.7min\n",
      "[CV 1/4] END max_depth=25, min_samples_leaf=50, min_samples_split=5, n_estimators=500; total time= 1.3min\n",
      "[CV 2/4] END max_depth=25, min_samples_leaf=50, min_samples_split=5, n_estimators=500; total time= 7.3min\n",
      "[CV 3/4] END max_depth=25, min_samples_leaf=50, min_samples_split=5, n_estimators=500; total time= 1.7min\n",
      "[CV 4/4] END max_depth=25, min_samples_leaf=50, min_samples_split=5, n_estimators=500; total time= 5.5min\n",
      "[CV 1/4] END max_depth=25, min_samples_leaf=50, min_samples_split=5, n_estimators=1000; total time= 1.2min\n",
      "[CV 2/4] END max_depth=25, min_samples_leaf=50, min_samples_split=5, n_estimators=1000; total time= 7.3min\n",
      "[CV 3/4] END max_depth=25, min_samples_leaf=50, min_samples_split=5, n_estimators=1000; total time= 1.8min\n",
      "[CV 4/4] END max_depth=25, min_samples_leaf=50, min_samples_split=5, n_estimators=1000; total time= 5.5min\n",
      "[CV 1/4] END max_depth=25, min_samples_leaf=50, min_samples_split=25, n_estimators=100; total time= 1.3min\n",
      "[CV 2/4] END max_depth=25, min_samples_leaf=50, min_samples_split=25, n_estimators=100; total time= 6.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/4] END max_depth=25, min_samples_leaf=50, min_samples_split=25, n_estimators=100; total time= 1.7min\n",
      "[CV 4/4] END max_depth=25, min_samples_leaf=50, min_samples_split=25, n_estimators=100; total time= 5.5min\n",
      "[CV 1/4] END max_depth=25, min_samples_leaf=50, min_samples_split=25, n_estimators=500; total time= 1.2min\n",
      "[CV 2/4] END max_depth=25, min_samples_leaf=50, min_samples_split=25, n_estimators=500; total time= 7.3min\n",
      "[CV 3/4] END max_depth=25, min_samples_leaf=50, min_samples_split=25, n_estimators=500; total time= 1.8min\n",
      "[CV 4/4] END max_depth=25, min_samples_leaf=50, min_samples_split=25, n_estimators=500; total time= 5.7min\n",
      "[CV 1/4] END max_depth=25, min_samples_leaf=50, min_samples_split=25, n_estimators=1000; total time= 1.3min\n",
      "[CV 2/4] END max_depth=25, min_samples_leaf=50, min_samples_split=25, n_estimators=1000; total time= 7.4min\n",
      "[CV 3/4] END max_depth=25, min_samples_leaf=50, min_samples_split=25, n_estimators=1000; total time= 1.9min\n",
      "[CV 4/4] END max_depth=25, min_samples_leaf=50, min_samples_split=25, n_estimators=1000; total time= 5.9min\n",
      "[CV 1/4] END max_depth=25, min_samples_leaf=50, min_samples_split=50, n_estimators=100; total time= 1.3min\n",
      "[CV 2/4] END max_depth=25, min_samples_leaf=50, min_samples_split=50, n_estimators=100; total time= 6.6min\n",
      "[CV 3/4] END max_depth=25, min_samples_leaf=50, min_samples_split=50, n_estimators=100; total time= 1.8min\n",
      "[CV 4/4] END max_depth=25, min_samples_leaf=50, min_samples_split=50, n_estimators=100; total time= 5.6min\n",
      "[CV 1/4] END max_depth=25, min_samples_leaf=50, min_samples_split=50, n_estimators=500; total time= 1.3min\n",
      "[CV 2/4] END max_depth=25, min_samples_leaf=50, min_samples_split=50, n_estimators=500; total time= 7.3min\n",
      "[CV 3/4] END max_depth=25, min_samples_leaf=50, min_samples_split=50, n_estimators=500; total time= 1.8min\n",
      "[CV 4/4] END max_depth=25, min_samples_leaf=50, min_samples_split=50, n_estimators=500; total time= 5.5min\n",
      "[CV 1/4] END max_depth=25, min_samples_leaf=50, min_samples_split=50, n_estimators=1000; total time= 1.3min\n",
      "[CV 2/4] END max_depth=25, min_samples_leaf=50, min_samples_split=50, n_estimators=1000; total time= 7.3min\n",
      "[CV 3/4] END max_depth=25, min_samples_leaf=50, min_samples_split=50, n_estimators=1000; total time= 1.7min\n",
      "[CV 4/4] END max_depth=25, min_samples_leaf=50, min_samples_split=50, n_estimators=1000; total time= 5.5min\n",
      "[CV 1/4] END max_depth=25, min_samples_leaf=100, min_samples_split=5, n_estimators=100; total time= 1.2min\n",
      "[CV 2/4] END max_depth=25, min_samples_leaf=100, min_samples_split=5, n_estimators=100; total time= 4.9min\n",
      "[CV 3/4] END max_depth=25, min_samples_leaf=100, min_samples_split=5, n_estimators=100; total time= 3.1min\n",
      "[CV 4/4] END max_depth=25, min_samples_leaf=100, min_samples_split=5, n_estimators=100; total time= 2.8min\n",
      "[CV 1/4] END max_depth=25, min_samples_leaf=100, min_samples_split=5, n_estimators=500; total time= 1.2min\n",
      "[CV 2/4] END max_depth=25, min_samples_leaf=100, min_samples_split=5, n_estimators=500; total time= 4.8min\n",
      "[CV 3/4] END max_depth=25, min_samples_leaf=100, min_samples_split=5, n_estimators=500; total time= 3.0min\n",
      "[CV 4/4] END max_depth=25, min_samples_leaf=100, min_samples_split=5, n_estimators=500; total time= 2.8min\n",
      "[CV 1/4] END max_depth=25, min_samples_leaf=100, min_samples_split=5, n_estimators=1000; total time= 1.2min\n",
      "[CV 2/4] END max_depth=25, min_samples_leaf=100, min_samples_split=5, n_estimators=1000; total time= 4.9min\n",
      "[CV 3/4] END max_depth=25, min_samples_leaf=100, min_samples_split=5, n_estimators=1000; total time= 3.0min\n",
      "[CV 4/4] END max_depth=25, min_samples_leaf=100, min_samples_split=5, n_estimators=1000; total time= 2.8min\n",
      "[CV 1/4] END max_depth=25, min_samples_leaf=100, min_samples_split=25, n_estimators=100; total time= 1.2min\n",
      "[CV 2/4] END max_depth=25, min_samples_leaf=100, min_samples_split=25, n_estimators=100; total time= 4.9min\n",
      "[CV 3/4] END max_depth=25, min_samples_leaf=100, min_samples_split=25, n_estimators=100; total time= 3.0min\n",
      "[CV 4/4] END max_depth=25, min_samples_leaf=100, min_samples_split=25, n_estimators=100; total time= 2.8min\n",
      "[CV 1/4] END max_depth=25, min_samples_leaf=100, min_samples_split=25, n_estimators=500; total time= 1.2min\n",
      "[CV 2/4] END max_depth=25, min_samples_leaf=100, min_samples_split=25, n_estimators=500; total time= 4.9min\n",
      "[CV 3/4] END max_depth=25, min_samples_leaf=100, min_samples_split=25, n_estimators=500; total time= 3.0min\n",
      "[CV 4/4] END max_depth=25, min_samples_leaf=100, min_samples_split=25, n_estimators=500; total time= 2.8min\n",
      "[CV 1/4] END max_depth=25, min_samples_leaf=100, min_samples_split=25, n_estimators=1000; total time= 1.2min\n",
      "[CV 2/4] END max_depth=25, min_samples_leaf=100, min_samples_split=25, n_estimators=1000; total time= 4.9min\n",
      "[CV 3/4] END max_depth=25, min_samples_leaf=100, min_samples_split=25, n_estimators=1000; total time= 3.0min\n",
      "[CV 4/4] END max_depth=25, min_samples_leaf=100, min_samples_split=25, n_estimators=1000; total time= 2.7min\n",
      "[CV 1/4] END max_depth=25, min_samples_leaf=100, min_samples_split=50, n_estimators=100; total time= 1.2min\n",
      "[CV 2/4] END max_depth=25, min_samples_leaf=100, min_samples_split=50, n_estimators=100; total time= 4.8min\n",
      "[CV 3/4] END max_depth=25, min_samples_leaf=100, min_samples_split=50, n_estimators=100; total time= 3.0min\n",
      "[CV 4/4] END max_depth=25, min_samples_leaf=100, min_samples_split=50, n_estimators=100; total time= 2.8min\n",
      "[CV 1/4] END max_depth=25, min_samples_leaf=100, min_samples_split=50, n_estimators=500; total time= 1.2min\n",
      "[CV 2/4] END max_depth=25, min_samples_leaf=100, min_samples_split=50, n_estimators=500; total time= 4.8min\n",
      "[CV 3/4] END max_depth=25, min_samples_leaf=100, min_samples_split=50, n_estimators=500; total time= 3.0min\n",
      "[CV 4/4] END max_depth=25, min_samples_leaf=100, min_samples_split=50, n_estimators=500; total time= 2.8min\n",
      "[CV 1/4] END max_depth=25, min_samples_leaf=100, min_samples_split=50, n_estimators=1000; total time= 1.2min\n",
      "[CV 2/4] END max_depth=25, min_samples_leaf=100, min_samples_split=50, n_estimators=1000; total time= 4.8min\n",
      "[CV 3/4] END max_depth=25, min_samples_leaf=100, min_samples_split=50, n_estimators=1000; total time= 3.0min\n",
      "[CV 4/4] END max_depth=25, min_samples_leaf=100, min_samples_split=50, n_estimators=1000; total time= 2.8min\n",
      "[CV 1/4] END max_depth=25, min_samples_leaf=250, min_samples_split=5, n_estimators=100; total time= 1.4min\n",
      "[CV 2/4] END max_depth=25, min_samples_leaf=250, min_samples_split=5, n_estimators=100; total time= 3.7min\n",
      "[CV 3/4] END max_depth=25, min_samples_leaf=250, min_samples_split=5, n_estimators=100; total time= 3.9min\n",
      "[CV 4/4] END max_depth=25, min_samples_leaf=250, min_samples_split=5, n_estimators=100; total time= 3.4min\n",
      "[CV 1/4] END max_depth=25, min_samples_leaf=250, min_samples_split=5, n_estimators=500; total time= 1.4min\n",
      "[CV 2/4] END max_depth=25, min_samples_leaf=250, min_samples_split=5, n_estimators=500; total time= 4.9min\n",
      "[CV 3/4] END max_depth=25, min_samples_leaf=250, min_samples_split=5, n_estimators=500; total time= 6.9min\n",
      "[CV 4/4] END max_depth=25, min_samples_leaf=250, min_samples_split=5, n_estimators=500; total time= 3.4min\n",
      "[CV 1/4] END max_depth=25, min_samples_leaf=250, min_samples_split=5, n_estimators=1000; total time= 1.4min\n",
      "[CV 2/4] END max_depth=25, min_samples_leaf=250, min_samples_split=5, n_estimators=1000; total time= 4.9min\n",
      "[CV 3/4] END max_depth=25, min_samples_leaf=250, min_samples_split=5, n_estimators=1000; total time= 6.9min\n",
      "[CV 4/4] END max_depth=25, min_samples_leaf=250, min_samples_split=5, n_estimators=1000; total time= 3.4min\n",
      "[CV 1/4] END max_depth=25, min_samples_leaf=250, min_samples_split=25, n_estimators=100; total time= 1.4min\n",
      "[CV 2/4] END max_depth=25, min_samples_leaf=250, min_samples_split=25, n_estimators=100; total time= 3.7min\n",
      "[CV 3/4] END max_depth=25, min_samples_leaf=250, min_samples_split=25, n_estimators=100; total time= 3.9min\n",
      "[CV 4/4] END max_depth=25, min_samples_leaf=250, min_samples_split=25, n_estimators=100; total time= 3.4min\n",
      "[CV 1/4] END max_depth=25, min_samples_leaf=250, min_samples_split=25, n_estimators=500; total time= 1.4min\n",
      "[CV 2/4] END max_depth=25, min_samples_leaf=250, min_samples_split=25, n_estimators=500; total time= 4.8min\n",
      "[CV 3/4] END max_depth=25, min_samples_leaf=250, min_samples_split=25, n_estimators=500; total time= 6.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/4] END max_depth=25, min_samples_leaf=250, min_samples_split=25, n_estimators=500; total time= 3.4min\n",
      "[CV 1/4] END max_depth=25, min_samples_leaf=250, min_samples_split=25, n_estimators=1000; total time= 1.4min\n",
      "[CV 2/4] END max_depth=25, min_samples_leaf=250, min_samples_split=25, n_estimators=1000; total time= 4.8min\n",
      "[CV 3/4] END max_depth=25, min_samples_leaf=250, min_samples_split=25, n_estimators=1000; total time= 6.9min\n",
      "[CV 4/4] END max_depth=25, min_samples_leaf=250, min_samples_split=25, n_estimators=1000; total time= 3.4min\n",
      "[CV 1/4] END max_depth=25, min_samples_leaf=250, min_samples_split=50, n_estimators=100; total time= 1.4min\n",
      "[CV 2/4] END max_depth=25, min_samples_leaf=250, min_samples_split=50, n_estimators=100; total time= 3.7min\n",
      "[CV 3/4] END max_depth=25, min_samples_leaf=250, min_samples_split=50, n_estimators=100; total time= 3.9min\n",
      "[CV 4/4] END max_depth=25, min_samples_leaf=250, min_samples_split=50, n_estimators=100; total time= 3.4min\n",
      "[CV 1/4] END max_depth=25, min_samples_leaf=250, min_samples_split=50, n_estimators=500; total time= 1.4min\n",
      "[CV 2/4] END max_depth=25, min_samples_leaf=250, min_samples_split=50, n_estimators=500; total time= 4.9min\n",
      "[CV 3/4] END max_depth=25, min_samples_leaf=250, min_samples_split=50, n_estimators=500; total time= 6.9min\n",
      "[CV 4/4] END max_depth=25, min_samples_leaf=250, min_samples_split=50, n_estimators=500; total time= 3.4min\n",
      "[CV 1/4] END max_depth=25, min_samples_leaf=250, min_samples_split=50, n_estimators=1000; total time= 1.4min\n",
      "[CV 2/4] END max_depth=25, min_samples_leaf=250, min_samples_split=50, n_estimators=1000; total time= 4.9min\n",
      "[CV 3/4] END max_depth=25, min_samples_leaf=250, min_samples_split=50, n_estimators=1000; total time= 6.9min\n",
      "[CV 4/4] END max_depth=25, min_samples_leaf=250, min_samples_split=50, n_estimators=1000; total time= 3.4min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11070697772300153"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name = 'tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_res_pos_2'\n",
    "orig_data, idxs, results = load_data_for_exp(exp_name)\n",
    "vectors = load_vectors_for_exp(exp_name)\n",
    "results['VycetVysledku'] = results['VycetVysledku'].fillna('') \n",
    "results['VahaEx'] = results['VycetVysledku'].apply(assign_weights)\n",
    "results['Vaha'] = results['VycetVysledku'].apply(lambda x: len(x.split(';')))\n",
    "\n",
    "model_name = 'fasttext_200_10'\n",
    "vector = vectors[model_name]\n",
    "X = vector[idxs]\n",
    "y = results['VahaEx'].to_numpy()\n",
    "\n",
    "if 'tf-idf' in model_name:\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "else:\n",
    "    scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "parmas = {'n_estimators': [100, 500, 1000], \n",
    "          'min_samples_split': [5, 25, 50], \n",
    "          'min_samples_leaf': [50, 100, 250],\n",
    "          'max_depth': [3, 10, 25]}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = GradientBoostingRegressor(random_state=42, n_iter_no_change=10)\n",
    "gs = GridSearchCV(model, parmas, verbose=3, cv=4)\n",
    "\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14400058907411029"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name = 'tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_res_pos_2'\n",
    "orig_data, idxs, results = load_data_for_exp(exp_name)\n",
    "vectors = load_vectors_for_exp(exp_name)\n",
    "results['VycetVysledku'] = results['VycetVysledku'].fillna('') \n",
    "results['VahaEx'] = results['VycetVysledku'].apply(assign_weights)\n",
    "results['Vaha'] = results['VycetVysledku'].apply(lambda x: len(x.split(';')))\n",
    "\n",
    "model_name = 'fasttext_200_10'\n",
    "vector = vectors[model_name]\n",
    "X = vector[idxs]\n",
    "y = results['Vaha'].to_numpy()\n",
    "\n",
    "if 'tf-idf' in model_name:\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "else:\n",
    "    scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "parmas = {'n_estimators': [100, 500, 1000], \n",
    "          'min_samples_split': [5, 25, 50], \n",
    "          'min_samples_leaf': [50, 100, 250],\n",
    "          'max_depth': [3, 10, 25]}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = GradientBoostingRegressor(random_state=42, n_iter_no_change=50, n_estimators=100, min_samples_split=5, min_samples_leaf=100, max_depth=50)\n",
    "\n",
    "model = model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015797000879581802"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name = 'tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_res_pos_2'\n",
    "orig_data, idxs, results = load_data_for_exp(exp_name)\n",
    "vectors = load_vectors_for_exp(exp_name)\n",
    "results['VycetVysledku'] = results['VycetVysledku'].fillna('') \n",
    "results['VahaEx'] = results['VycetVysledku'].apply(assign_weights)\n",
    "results['Vaha'] = results['VycetVysledku'].apply(lambda x: len(x.split(';')))\n",
    "\n",
    "model_name = 'fasttext_200_10'\n",
    "vector = vectors[model_name]\n",
    "X = vector[idxs]\n",
    "y = results['Vaha'].to_numpy()\n",
    "\n",
    "if 'tf-idf' in model_name:\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "else:\n",
    "    scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "parmas = {'n_estimators': [100, 500, 1000], \n",
    "          'min_samples_split': [5, 25, 50], \n",
    "          'min_samples_leaf': [50, 100, 250],\n",
    "          'max_depth': [3, 10, 25]}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = SVR(kernel='rbf', cache_size=1000, epsilon=.01, shrinking=False, C=.99)\n",
    "\n",
    "model = model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2107702671263345"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name = 'tf_w2v_d2v_fast_cz_nolemma_naz_anot_obor_uc_res_pos_2'\n",
    "orig_data, idxs, results = load_data_for_exp(exp_name)\n",
    "vectors = load_vectors_for_exp(exp_name)\n",
    "results['VycetVysledku'] = results['VycetVysledku'].fillna('') \n",
    "results['VahaEx'] = results['VycetVysledku'].apply(assign_weights)\n",
    "results['Vaha'] = results['VycetVysledku'].apply(lambda x: len(x.split(';')))\n",
    "\n",
    "model_name = 'fasttext_200_10'\n",
    "vector = vectors[model_name]\n",
    "X = vector[idxs]\n",
    "y = results['Vaha'].to_numpy()\n",
    "\n",
    "if 'tf-idf' in model_name:\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "else:\n",
    "    scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "parmas = {'n_estimators': [100, 500, 1000], \n",
    "          'min_samples_split': [5, 25, 50], \n",
    "          'min_samples_leaf': [50, 100, 250],\n",
    "          'max_depth': [3, 10, 25]}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = MLPRegressor(hidden_layer_sizes=(256, 64), random_state=42)\n",
    "\n",
    "model = model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr = results.rename(columns={'VycetVysledku': 'Výčet druhů výsledků', 'Vaha': 'Bodové ohodnocení', 'VahaEx': 'Vážené bodové ohodnocení'}).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "dfr = dfr[['Název česky', 'Výčet druhů výsledků', 'Bodové ohodnocení']]\n",
    "dfr['Predikované bodové ohodnocení'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Název česky</th>\n",
       "      <th>Výčet druhů výsledků</th>\n",
       "      <th>Bodové ohodnocení</th>\n",
       "      <th>Predikované bodové ohodnocení</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kód projektu</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GA203/98/1330</th>\n",
       "      <td>Uplatnění (CH2-O), (CH2-S) a (CH2-CH2) isosterů amidové a disulfidové vazby v syntéze bioaktivních peptidů s prodlouženým účinkem</td>\n",
       "      <td>D ;D ;D</td>\n",
       "      <td>3</td>\n",
       "      <td>4.330835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IA31213</th>\n",
       "      <td>Hlubinná geoelektrická stavba jihozápadního okraje Českého masivu</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1.555721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GA522/02/1227</th>\n",
       "      <td>Regulační aspekty přímé somatické embryogeneze u hrachu (Pisum sativum L.)</td>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "      <td>0.474226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GA102/96/1610</th>\n",
       "      <td>Nové technologie přípravy optoelektronických struktur</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>8.022363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LTC17067</th>\n",
       "      <td>Využití NMR relaxačních a difúzních měření pro stanovení dynamiky komplexních molekulárních systémů</td>\n",
       "      <td>Jimp ;Jimp ;Jimp ;Jimp ;Jimp ;Jimp</td>\n",
       "      <td>6</td>\n",
       "      <td>3.754150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GA203/95/0650</th>\n",
       "      <td>Chemické aplikace indexů podobnosti</td>\n",
       "      <td>Jx ;Jx ;Jx ;D ;Jx ;Jx ;D ;D ;Jx ;Jx ;Jx ;Jx</td>\n",
       "      <td>12</td>\n",
       "      <td>13.377961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GA103/02/0243</th>\n",
       "      <td>Odstraňování huminových látek z přírodních vod</td>\n",
       "      <td>D ;D ;D ;D ;D ;D ;D ;D ;D ;D ;D ;D ;Jx ;D ;D ;D ;D ;D ;A ;A ;D ;O ;Jx ;D ;D ;A ;D ;D ;D ;D ;D ;C</td>\n",
       "      <td>32</td>\n",
       "      <td>19.685251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GA526/03/1485</th>\n",
       "      <td>Produkční potenciál a stabilita smíšených lesních porostů ve 2., 3. a 4. lesním vegetačním stupni jako podklad pro optimalizaci cílové skladby dřevin</td>\n",
       "      <td>Jx ;Jx ;D ;D ;D ;Jx ;D ;D ;D ;Jx</td>\n",
       "      <td>10</td>\n",
       "      <td>10.307500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VS97046</th>\n",
       "      <td>Centrum mikrosystémů</td>\n",
       "      <td>D ;Jx ;D ;D ;O ;O</td>\n",
       "      <td>6</td>\n",
       "      <td>8.889691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GA312/95/0583</th>\n",
       "      <td>Fenotypická reverse savčí buňky transformované onkogenem v-src - model pro studium buněčných mechanismů regulace provirové exprese</td>\n",
       "      <td>Jx ;Jx</td>\n",
       "      <td>2</td>\n",
       "      <td>3.623506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                         Název česky  \\\n",
       "Kód projektu                                                                                                                                                           \n",
       "GA203/98/1330                      Uplatnění (CH2-O), (CH2-S) a (CH2-CH2) isosterů amidové a disulfidové vazby v syntéze bioaktivních peptidů s prodlouženým účinkem   \n",
       "IA31213                                                                                            Hlubinná geoelektrická stavba jihozápadního okraje Českého masivu   \n",
       "GA522/02/1227                                                                             Regulační aspekty přímé somatické embryogeneze u hrachu (Pisum sativum L.)   \n",
       "GA102/96/1610                                                                                                  Nové technologie přípravy optoelektronických struktur   \n",
       "LTC17067                                                         Využití NMR relaxačních a difúzních měření pro stanovení dynamiky komplexních molekulárních systémů   \n",
       "GA203/95/0650                                                                                                                    Chemické aplikace indexů podobnosti   \n",
       "GA103/02/0243                                                                                                         Odstraňování huminových látek z přírodních vod   \n",
       "GA526/03/1485  Produkční potenciál a stabilita smíšených lesních porostů ve 2., 3. a 4. lesním vegetačním stupni jako podklad pro optimalizaci cílové skladby dřevin   \n",
       "VS97046                                                                                                                                         Centrum mikrosystémů   \n",
       "GA312/95/0583                     Fenotypická reverse savčí buňky transformované onkogenem v-src - model pro studium buněčných mechanismů regulace provirové exprese   \n",
       "\n",
       "                                                                                            Výčet druhů výsledků  \\\n",
       "Kód projektu                                                                                                       \n",
       "GA203/98/1330                                                                                           D ;D ;D    \n",
       "IA31213                                                                                                            \n",
       "GA522/02/1227                                                                                                 D    \n",
       "GA102/96/1610                                                                                                      \n",
       "LTC17067                                                                     Jimp ;Jimp ;Jimp ;Jimp ;Jimp ;Jimp    \n",
       "GA203/95/0650                                                       Jx ;Jx ;Jx ;D ;Jx ;Jx ;D ;D ;Jx ;Jx ;Jx ;Jx    \n",
       "GA103/02/0243  D ;D ;D ;D ;D ;D ;D ;D ;D ;D ;D ;D ;Jx ;D ;D ;D ;D ;D ;A ;A ;D ;O ;Jx ;D ;D ;A ;D ;D ;D ;D ;D ;C    \n",
       "GA526/03/1485                                                                  Jx ;Jx ;D ;D ;D ;Jx ;D ;D ;D ;Jx    \n",
       "VS97046                                                                                       D ;Jx ;D ;D ;O ;O    \n",
       "GA312/95/0583                                                                                            Jx ;Jx    \n",
       "\n",
       "               Bodové ohodnocení  Predikované bodové ohodnocení  \n",
       "Kód projektu                                                     \n",
       "GA203/98/1330                  3                       4.330835  \n",
       "IA31213                        1                       1.555721  \n",
       "GA522/02/1227                  1                       0.474226  \n",
       "GA102/96/1610                  1                       8.022363  \n",
       "LTC17067                       6                       3.754150  \n",
       "GA203/95/0650                 12                      13.377961  \n",
       "GA103/02/0243                 32                      19.685251  \n",
       "GA526/03/1485                 10                      10.307500  \n",
       "VS97046                        6                       8.889691  \n",
       "GA312/95/0583                  2                       3.623506  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfr.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
